%!TEX root = ../main.tex

\chapter{Introduction}

In this thesis I will show how we achieve type-safe access in the Rust programming language to data encoded in JSON format, provided that we at compile time (or earlier) have access to a sample with the same structure as the data we wish to read at runtime.

This is achieved by drawing inspiration from a feature of the F\# programming languages called type providers and approximating this feature in Rust with -- among other tools -- a procedural macro.

\section{The Rust programming language}
\label{sec:rust-intro}

The Rust programming language is a modern, open source, systems programming language with C-like surface syntax. It is a statically typed language with type inference, with a focus on memory safety and zero-cost abstractions. It does not use a garbage collector and the Rust core library can be used without access to heap allocation.

While the basic syntax of Rust can be said to be C-like it also has a lot of functionality and syntax reminiscent of functional languages and in particular languages of the ML-family. Features such as closures, pattern matching and monadic error handling are available and a significant part of idiomatic Rust code.

What follows is a very basic introduction to the parts of Rust we need to talk about in this thesis. For more details see \href{https://doc.rust-lang.org/book/}{the Rust book}\footnote{\url{https://doc.rust-lang.org/book/}}.

\subsection{Structs}

\begin{listing}[ht!]
\begin{minted}{rust}
struct Person {
    name: String,
    age: i64,
}
\end{minted}
\caption{A basic Rust struct}
\label{lst:struct}
\end{listing}

A struct declaration in Rust is a number of fields with types. Also known as a product type or record. Rust is not object oriented, and there is no inheritance for structs. Polymorphism in Rust is instead supported by enumerated types and traits.

\subsection{Enums}

\begin{listing}[ht!]
\begin{minted}{rust}
enum Option<T> {
    None,
    Some(T),
}
\end{minted}
\caption{The enumerated type ‹Option› in Rust}
\label{lst:enum}
\end{listing}

An enum in Rust is a data type that is one alternative from a number of variants, i.e. an enumerated type, sum type or union. A declaration of an enum, as seen in listing~\ref{lst:enum} is given as a list of variant names, used as constructors. Each variant can optionally carry some associated data.

Rust supports pattern matching on enum types and verifies that any match is exhaustive.

\subsection{Traits}
\label{sec:traits}

A trait specifies methods that a type needs to provide to implement the trait.

Some traits can be derived -- automatically implemented -- by the compiler. This is done by annotating the type with a ‹derive› attribute with a list of the traits to derive implementations for.

\begin{listing}[ht!]
\begin{minted}{rust}
trait Clone {
    fn clone(&self) -> Self;
}

#[derive(Debug)]
struct Bag {
    label: String
}

impl Clone for Bag {
    fn clone(&self) -> Self {
        Bag {
          label: self.label.clone()
        }
    }
}
\end{minted}
\caption{The Rust trait ‹Clone› and examples of implementation}
\label{lst:trait}
\end{listing}

Listing~\ref{lst:trait} shows a simplified version of the trait ‹Clone›, a struct which automatically derives the trait ‹Debug› -- with the annotation ‹derive(Debug)› -- and manually implements ‹Clone› with an ‹impl Clone› block.

It is also possible to write libraries for deriving custom traits. Deriving a trait usually requires that the constituent types also implements the trait. E.g. to derive ‹Clone› for ‹Ty› you need to be able to clone all parts of ‹Ty›.

\subsection{Macros}
\label{sec:macros}

A macro system is in short a language feature that allows a programmer to write code that does source-to-source transformations, also known as meta-programming. In Rust there are two categories of macros: declarative macros and procedural macros, which are both expanded at compile time.

\begin{listing}[ht!]
\textbf{Macro declaration:}
\begin{minted}{rust}
macro_rules! double {
    ($e: expr) => ({
        let temp = $e;
        temp + temp
    })
}
\end{minted}
\vspace{5mm}

\textbf{Macro usage:}
\begin{minted}{rust}
let ten = double!(2 + 3);
\end{minted}
\caption{A simple declarative macro in Rust}
\label{lst:macro-rules}
\end{listing}

Declarative macros in Rust are also known as \say{macros by example} or \say{pattern-based macros}. These are syntactic, hygienic macros that with a syntax similar to pattern matching, match input patterns to expanded source code. Listing~\ref{lst:macro-rules} show declaration and usage of a very basic declarative macro. The macro ‹double› has a single rule transforming a single expression to a block expression. A macro can have multiple rules and each pattern can be a combination of tokens and metavariables.

Procedural macros are macros where the expansion is done by running a procedure rather than by evaluating rules. To create a procedural macro in Rust you create a library that exposes a function that takes a ‹TokenStream› as input and outputs a ‹TokenStream›. Since this library and function can use arbitrary Rust code to generate its output, it can do things that are outside the scope of the normal compiler. Examples of things there are already procedural macro libraries for are:

\begin{itemize}
  \item Generating a type-safe DSL (at compile time) for communicating with an SQL database, by inspecting said database.\cite{diesel}
  \item Compiling graphics shaders at Rust compile time.\cite{vulkano}
\end{itemize}

% Possible expansions here:
% - Current state of macros in Rust. Past, future.

\subsection{Cargo}

While strictly speaking not a part of the Rust programming language, Cargo comes with every normal Rust installation. Cargo is the package manager and build tool for the Rust ecosystem.

For developers from languages with strong tooling support this may not seem like a very big deal. However, for both C and C++ adding a dependency to a project is a major decision due to the burden it places on developers and sometimes even users.

\placeholder{Explain crates here} ... A crate is the equivalent of a library or a package in Cargo, Rust's package manager.

% Possible expansions here:
% - Example of Cargo.toml and cargo commands
% - Sentence about crates.io

\section{JSON}
\label{sec:json}

JSON is a text-based data format for structured data. It is very commonly used as a data interchange format in modern HTTP-based architectures. JSON was originally based on a subset of the JavaScript programming language, but is designed to be language-independent and is now used in the interaction between applications written in practically all programming languages.

\begin{listing}[ht!]
\begin{align*}
\textit{value} ::=  &\ \textit{object} \mid \textit{array} \mid \textit{number} \mid \textit{string} \mid ‹true› \mid ‹false› \mid ‹null› \\
\textit{object} ::=  &\ \icode{\{}\ [\ \textit{string}\ ‹:›\ \textit{value}\ \text{*}(\ ‹,›\ \textit{string}\ ‹:›\ \textit{value}\ )\ ]\ \icode{\}} \\
\textit{array} ::= &\ ‹[›\ [\ \textit{value}\ \text{*}(\ ‹,›\ \textit{value}\ )\ ]\ ‹]› \\
\textit{number} ::= &\ [\ ‹-›\ ]\ \textit{int}\ [\ \textit{frac}\ ]\ [\ \textit{exp}\ ]\ \\
\textit{string} ::= &\ ‹"›\ \text{*}\textit{char}\ ‹"›
\end{align*}
\caption{The JSON grammar from RFC 7159. It is somewhat simplified as the actual specification is very precise. See the full specification for the exact definitions of \textit{int}, \textit{frac}, \textit{exp} and \textit{char}.}
\label{lst:json-grammar}
\end{listing}

The format as described by the two official JSON specifications -- IETF RFC 7159\cite{RFC7159} and ECMA 404\cite{ECMA404} -- is very simple. As can be seen from the grammar in listing~\ref{lst:json-grammar} a JSON value can only be an object, array, number or string, or the literals ‹true›, ‹false› or ‹null›. In addition, there is no way to define new types, references or to specify external resources or definitions.
% contrast simplicity to XML?

% FIXME: The whole misnomer thing is a bit iffy
A JSON object is a collection of mappings from keys (that are JSON strings) to JSON values. As this closely corresponds to what is usually referred to as a map or a dictionary in most programming languages, the term object may seem like a bit of a misnomer here, arising from JSONs background as a subset of JavaScript. Whether or not an object is an unordered is explicitly unspecified by RFC 1759, but \say{Implementations whose behavior does not depend on member ordering will be interoperable in the sense that they will not be affected by these differences.}\cite[6]{RFC7159}

A JSON array is an ordered collection of JSON values. It is worth noting that JSON places no restriction on the types of the values of the array, and as such, it can sometimes be a heterogeneous collection.

\begin{listing}[ht!]
\begin{minted}{json}
{
  "name": "Bob",
  "age": 24,
  "phoneNumbers": [
    {
      "areaCode": 456,
      "number": 80931
    }
  ]
}
\end{minted}
\caption{An example JSON object}
\label{lst:json1}
\end{listing}

Listing~\ref{lst:json1} shows a basic example of a JSON object. Since JSON objects and arrays can themselves contain objects and arrays, data serialized as JSON can be arbitrarily deeply nested, but as mentioned JSON itself has no support for any kind of references, so there is no possibility of loops or infinite types.

\placeholder{A JSON number...}

\placeholder{A JSON string...}

% Everything is "nullable"

\subsection{Reading JSON in dynamically typed languages}

\begin{listing}[ht!]
\begin{minted}{js}
var parsed = JSON.parse(jsonString);
console.log(parsed.phoneNumbers[0].areaCode);
\end{minted}
\caption{Printing the first areaCode in JavaScript}
\label{lst:readjsonjs}
\end{listing}

\begin{listing}[ht!]
\begin{minted}{python}
parsed = JSON.load(jsonString)
print(parsed.phoneNumbers[0].areaCode)
\end{minted}
\caption{Printing the first areaCode in Python}
\label{lst:readjsonpy}
\end{listing}

In dynamically typed programming languages reading data from deserialized JSON is relatively straightforward, as seen in listings~\ref{lst:readjsonjs} and~\ref{lst:readjsonpy}. The code in these listings can, however, fail at any point if the deserialized data does not match expectations, so in many real world use cases the code will be a bit more complex.

\subsection{Reading JSON in statically typed languages}

In statically typed programming languages reading data from deserialized JSON often requires a bit more effort. There are several possible (and relatively common) approaches.

For my examples I will use Rust, but first I need to introduce «Serde»:

\subsubsection{Serde}
\label{sec:serde}

«Serde» is a Rust framework for serialization and deserialization. The core of the framework is independent of the source/target data format, and the input/output of specific data formats is provided by separate crates.

«Serde» works by providing two traits, ‹Serialize› and ‹Deserialize›, that Rust types can implement. The core library has already implemented these traits for the most common data types like ‹i64›, ‹String›, ‹Vec› and ‹HashMap›. In addition, the crate «serde_derive» provides code for deriving ‹Serialize› and ‹Deserialize› for custom -- i.e. your own -- types.

Once a Rust type implements the necessary trait, data can be converted with the various format specific crates, that provide implementations for one or both of the traits «Serializer» and «Deserializer». «serde_json» provides implementations of these types for JSON. It also provides some other functionality that is useful for working with JSON.

\subsubsection{Catchall types}

\begin{listing}[ht!]
\begin{minted}{rust}
enum Value {
    Null,
    Bool(bool),
    Number(Number),
    String(String),
    Array(Vec<Value>),
    Object(Map<String, Value>),
}
\end{minted}
\caption{An enumerated type in Rust for JSON values}
\label{lst:valueenum}
\end{listing}

Listing~\ref{lst:valueenum} shows the Rust type ‹Value› from the library «serde_json». Any legal JSON can be encoded by this type. Similar types can be created in most programming languages. However, working with such types in a type-safe manner can be both tedious and error-prone. The code in listing~\ref{lst:readjsonrs1} does approximately the same thing as the code in listings~\ref{lst:readjsonjs} and~\ref{lst:readjsonpy}, but is quite clearly a lot more complicated. It does have one advantage though, no possibility of a runtime error when accessing the data.

\begin{listing}[ht!]
\begin{minted}{rust}
let parsed: Value = serde_json::from_str(json_str)?;

if let Some(array) = parsed.get("phoneNumbers") {
    if let Some(obj) = array.get(0) {
        if let Some(num) = obj.get("areaCode") {
            println!("{}", num);
        }
    }
}
\end{minted}
\caption{Printing the first areaCode in Rust}
\label{lst:readjsonrs1}
\end{listing}

% if let Some(num) = parsed.get("phoneNumbers")
%                          .and_then(|v| v.get(0))
%                          .and_then(|v| v.get("areaCode")) {
%     println!("{}", num);
% }

\subsubsection{Stringly typed code}

One way to make the catchall types easier to work with is to create helpers that essentially eschew the advantages of type checking, one way or the other. Listing~\ref{lst:readjsonrs2} shows two alternative ways to read from the parsed data. \morestuff

\begin{listing}[ht!]
\begin{minted}{rust}
let parsed: Value = serde_json::from_str(json_str)?;

println!("{}", parsed["phoneNumbers"][0]["areaCode"]);

if let Some(num) = parsed.pointer("/phoneNumbers/0/areaCode") {
    println!("{}", num);
}
\end{minted}
\caption{Printing the first areaCode in Rust}
\label{lst:readjsonrs2}
\end{listing}

\subsubsection{Custom types}

What we actually want are custom types, that accurately reflects the actual data. This way, any mismatches between our assumptions and the actual structure of the data is revealed at the time of parsing. And accessing any particular field is just as type-safe as any other native data in the programming language.

Listing~\ref{lst:readjsonrs3} shows an example of using such custom data types with the «serde» framework. However, even for this small example this approach required 9 significant lines of code for the types. And with real-world data sources, actual JSON data can be significantly larger and more complex. So while custom types are preferable once written, actually writing them can often be a significant hurdle.

\begin{listing}[ht!]
\begin{minted}{rust}
#[derive(Deserialize)]
struct Person {
    name: String,
    age: i64,
    phoneNumbers: Vec<PhoneNumber>,
}

#[derive(Deserialize)]
struct PhoneNumber {
    areaCode: i64,
    number: i64,
}

let parsed: Person = serde_json::from_str(json_str)?;

println!("{}", parsed.phoneNumbers[0].areaCode);
\end{minted}
\caption{Printing the first areaCode in Rust}
\label{lst:readjsonrs3}
\end{listing}

\section{Type providers}

Solving this problem is of course something that has been thought about before. F\# is an open source, statically typed, functional programming language for the Common Language Runtime with an ML-like surface syntax. In version 3.0 of F\# a concept called \say{type providers} was introduced.

A type provider provides convenient, type-safe access to an external, potentially complex resource, in a statically typed programming language. As the name suggests the type provider \say{provides} the necessary custom types, letting the programmer access the resource without having to spell out the type information.

\begin{listing}[ht!]
\begin{minted}{fsharp}
open FSharp.Data

type Simple = JsonProvider<""" { "name":"John", "age":94 } """>

[<EntryPoint>]
let main argv =
    let simple = Simple.Parse(""" { "name":"Tomas", "age":4 } """)
    printfn "%s %d" simple.Name simple.Age
    // prints "Tomas 4"
    0
\end{minted}
\caption{Minimal example of the use of a type provider in F\#}
\label{lst:fsharpsample}
\end{listing}

Listing~\ref{lst:fsharpsample} shows an example of a type provider, ‹JsonProvider›, in use. The type provider is given a static parameter, in this case a string of JSON. A type is generated from this sample at compile time, and the type is then used to parse another instance, with the same structure, at runtime. The parsed object is then available to be used in a type-safe manner, just like any other instance of a custom type.

To reiterate: the expression ‹JsonProvider<...>› is evaluated at compile time and provides a type to the compiler. This lets the compiler know the type of, and type check, expressions like ‹simple.Age›, even though the type, or even the existence, of the field ‹Age› is not visible anywhere in the actual F\# code. This is somewhat reminiscent of how type inference lets the compiler lets the compiler know the type of a variable like ‹name› in an expression like ‹let name = "Bob"›, even if the type is not visible in code.

The particular type provider shown, ‹JsonProvider›, comes from the library \fsharpdata\cite{fsharpdata}, which has type providers for multiple formats, and the example is adapted from its documentation\footnote{\url{http://fsharp.github.io/FSharp.Data/library/JsonProvider.html}}. Both the sample string and the string providing the instance could have been replaced by paths to either local or remote resources:

\begin{minted}{fsharp}
type Simple = JsonProvider<"http://example.com/person.json">
type Simple = JsonProvider<"samples/person.json">
\end{minted}

The compiler also provides functionality that makes it easy for implementors of editors to provide autocomplete and similar functionality for types generated by type providers. Thus, when writing code that talks to a JSON based API, it is often possible to just point the type provider to an endpoint or a sample from some documentation, and get the types one needs to start working. Possibly even without looking at the JSON, and instead using autocomplete to explore the data.

\placeholder{Autocomplete screenshot of simple.}

\subsection{Type providers from an implementation standpoint}

So what \emph{is} a type provider? How do they work? From an implementation standpoint, support for type providers can be boiled down to two features:

\begin{itemize}
  \item Compile-time meta-programming, with the flexibility of a full programming language.
  \item Support for thunks, i.e. lazy or delayed evaluation, of at least parts of the abstract syntax tree, and letting such thunks be made by compile-time meta-programming.
\end{itemize}

Additionally, for a type provider to make sense the language has to be statically typed. In a dynamically typed language, any type checking is done at runtime, and is tied to actual instances, so providing extra type information before runtime does not provide much benefit. Since type providers can create a large number of types it is also beneficial if the language has type inference so the actual names of the sub-types are less significant.

As we looked at in section~\ref{sec:macros}, Rust does have compile-time meta-programming with the flexibility of a full programming language, in the form of procedural macros. It is also a statically typed language with type inference. Rust does however not have any ability for procedural macros to let parts of the generated code be created lazily.

In F\# type provider implementations it is possible to add type members as thunks, which are then only evaluated as required by the compiler. This makes it possible for type providers to provide access to data which whose type spaces would otherwise be too large to create custom types for. Just like lazy evaluation makes infinite lists possible, and practical, in languages like Haskell, thunks as parts of the generated types makes arbitrary large or infinite type hierarchies possible, and practical, in F\#.

A good example of a type provider which makes use of this functionality is the World Bank type provider in \fsharpdata.

\placeholder{\fsharpdata\ World Bank provider example and explanation.}

While thunks as parts of generated types is clearly a very powerful feature, many type providers are not reliant on it. If it is feasible to have all the necessary data available at once, and the generated type is not too large, delayed production of the code is not required. For example the ‹JsonProvider› from \fsharpdata\ which creates a single type hierarchy from a limited set of samples, does not need thunks. In other words, something like type providers like ‹JsonProvider› \emph{should} be possible in Rust. In chapter~\ref{chap:project-presentation} I will show how I have attempted to achieve this, but let us first look at the advantages and disadvantages of the type providers it is inspired by.

\subsection{Advantages of F\#s type providers}

% "Type checking" external resources
The most significant benefit of type providers is that they enable type safe access to external resources, significantly improving type checking for sections of code using data from a type provider.

% Explorative programming / Tooling help
As mentioned, such type safe access can also be achieved by manually writing the custom types, but the ease of use of a type provider makes it very easy to add a new external resource and start experimenting. Not only is the barrier of having to write boilerplate code reduced or removed, but type information gleaned through e.g. autocomplete facilitates exploration of the resource.

When working with external data, the code we write encode our assumptions about the data we are working with. E.g. if I have parsed some data into a variable ‹p› and I write code like ‹let real_name: String = p.name›, the code encodes my assumptions that the data has a field with the name ‹name›, and that said field is a string. When the types are in sync with the external data the type checking will thus check that our assumptions match the actual structure of the data. Additionally, the type checking will \emph{only} check the assumptions we state through our use of the data, which are likely to be the ones we care about.

% Detect API changes
Since these assumptions are re-checked when we re-compile this can help us not just when our assumptions change, but also when the things we are making assumptions about change. In this way a type provider like ‹JsonProvider› can help alert us about changes to remote resources while we are developing or in a continuous  integration setting.

% Encoding of otherwise impractical/impossible types
The most advanced type providers can thanks to the delayed evaluation of AST encode information that would otherwise be entirely impractical to otherwise encode in the type system. We will be hard pressed to recreate this in Rust since we will not have access to delayed evaluation, but there may be other ways that we may be able to make different trade-offs and thus explore new possibilities.

\subsection{Disadvantages of F\#s type providers}
\label{sec:disadvantages-of-type-providers}

While F\#s type providers have significant advantages, they also have some disadvantages. Type providers are essentially a code generation tool. However, type providers do not give the user access to the generated code. For type providers that use thunks, it is of course not a real option to generate the full code to be able to show it, as the \say{full} code could in fact be infinite. And if you just show the code that is already forced by the program usage, no new information is provided.

% Understanding code you cannot see

Since the difference between type providers that use delayed evaluation and those that don't is completely abstracted away from the users of the type providers, this means that there is no option to look at the generated code to understand what is going on and to inspect the structure of the parsed data. While some of the same understanding can be achieved by looking at the autocomplete suggestions it is often easier to understand the structure by looking at the actual types.

% Migration path

No access to the generated code also causes a certain amount of lock-in. Without access to the code, trying to migrate a project using type providers to manually written types forces a complete rewrite of the types provided by the type provider. In the case of parsing JSON and generating types from it there are many trade-offs that have to be made, and thus there might be reasons for such a migration as a project develops. I will come back to these trade-offs in section~\ref{sec:design-considerations}.

% Network access
A completely different issue with certain type providers is that they provide the greatest benefit with concern to the type checking when verifying the use of an external resource. However, if access to this external resource requires network access, this causes the compilation of the program to require network access for every build. Sometimes requiring network access to build is not a big concern, but other times it can be a big annoyance or even be completely out of the question.

% Build can break without code changes
Another concern many people have with type providers is that since the type checking checks if the code matches the data, the build can break if the structure of the data changes. In other words, the advantage of having our assumptions checked by the type checking also means that the build can break without any changes to the things we manage with version control. Such breakage runs contrary to the concept of reproducible builds which is a very common goal for build systems. One may counter that an external data source like an API is part of the system when it is used in a program, and as such a build failure is the correct behavior, but it is nevertheless a contentious issue.

While these very real concerns are not something that dissuade us from pursuing the significant benefits that type providers can give us, we should at the same time keep them in mind as we go forward.
