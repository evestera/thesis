%!TEX root = ../main.tex

\chapter{Code generation}
\label{sec:code-generation}

Once a JSON sample has been retrieved and parsed using «serde_json» into the catchall type ‹serde_json::Value›, «json_typegen» is ready to begin inference and code generation. In this chapter we will look at the details of these three stages:

\begin{itemize}
  \item First a generalized \say{shape} is inferred from the JSON values.
  \item Then this inference is enhanced in intermediary passes.
  \item Finally Rust code is generated based on the inferred shapes.
\end{itemize}

First I will present the basic version of the inference algorithm and code generation, before looking at how this basic system can be extended in various ways.

\section{Shape inference}

The shape inference is based on the algorithms used in \fsharpdata\ as presented in the paper \emph{Types from Data: Making Structured Data First-class Citizens in F\#}\cite{fsharp-types-from-data}. As done in this paper I will use the term shape for the intermediate representation of the inference data, to avoid confusion with actual programming language types.

% Describe how shapes and types differ

The basic algorithm has two main functions, $\shap$ and $\csh$.

The function $\shap$, shown in figure~\ref{fig:shap} takes as its input the parsed ‹serde_json::Value› and produces a shape. The catch-all ‹serde_json::Value› as was shown in listing~\ref{lst:valueenum} can be written, with value variables \omega, as:

\begin{align*}
\omega ::=  &\ ‹Null› \mid ‹Bool(›b‹)› \mid ‹Number(›n‹)› \mid ‹String(›s‹)› \\ \mid &\ ‹Array(›[\omega_1, \cdots, \omega_n]‹)› \mid ‹Object(›\{ k_1 : \omega_1, \cdots, k_n : \omega_n \}‹)›
\end{align*}

$\shap$ maps these values to the generalized shapes. Our shapes, with shape variables $\sigma$, are as follows:

\begin{align*}
\sigma ::=  &\ ‹null› \mid ‹bool› \mid ‹any› \mid \bot \mid ‹string› \mid ‹int› \mid ‹float› \\
       \mid &\ ‹optional(›\sigma‹)› \mid \vect{\sigma} \mid \recd{ k_1 : \sigma_1, \cdots, k_n : \sigma_n }
\end{align*}

To be clear about how notation is disambiguated: $[a, b, c]$ is an actual sequence of items, while $\vect{a}$ is the \emph{shape} representing a sequence/list. Likewise $\{ k_1 : v_1, \cdots, k_n : v_n \}$ is a map-like collection, while $\recd{ k_1 : \sigma_1, \cdots, k_n : \sigma_n }$ is the \emph{shape} representing a record/object/struct.

\begin{figure}[ht!]
\begin{align*}
\shap(‹Null›)          &= ‹null› \\
\shap(‹Bool(›b‹)›)     &= ‹bool› \\
\shap(‹Number(›n‹)›)   &= \left\{\begin{array}{ll}
  ‹int›   & \text{if } n \in \mathbb{Z} \\
  ‹float› & \text{otherwise}
\end{array}\right.\\
\shap(‹String(›s‹)›)   &= ‹string› \\
\shap(‹Array(›[\omega_1, \cdots, \omega_n]‹)›) &= \vect{ \fold(\csh, \bot, [\shap(\omega_1), \cdots, \shap(\omega_n)]) } \\
\shap(‹Object(›\{ k_1 : \omega_1, \cdots, k_n : \omega_n \}‹)›) &= \recd{ k_1 : \shap(\omega_1), \cdots, k_n : \shap(\omega_n) }
\end{align*}
\caption{$\shap$, the function mapping JSON values to shapes}
\label{fig:shap}
\end{figure}

% Differences from algorithm as presented in Types from Data
Unlike in «Types from Data», to preserve information and let the choice be made in the code generation step, lists are not considered nullable. In the default code generation, ‹Option<Vec<T>>› will not be generated, but this can be enabled.

\begin{figure}[ht!]
\begin{align*}
\csh(\sigma, \sigma)               &=  \sigma               & (eq) \\
\csh(\sigma, \bot)            &=  \sigma               & (bottom) \\
\csh(‹int›, ‹float›)     &= ‹float›          & (num) \\
\csh(\sigma, ‹null›)          &= \opt(\sigma)          & (null) \\
\csh(\sigma_1, ‹optional(›\sigma_2‹)›) &= \opt(\csh(\sigma_1, \sigma_2)) & (opt) \\
\csh(\vect{\sigma_1}, \vect{\sigma_2})           &= \vect{\csh(\sigma_1, \sigma_2)}     & (arr) \\
\csh(\sigma_1 = \recd{ \cdots }, \sigma_2 = \recd{ \cdots }) &= \cfs(\sigma_1, \sigma_2) & (obj) \\
\csh(\sigma_1, \sigma_2)               &= ‹any›            & (any)
\end{align*}
\caption{$\csh$, the common shape function}
\label{fig:csh}
\end{figure}

\begin{figure}[ht!]
\begin{align*}
\opt(‹null›) &= ‹null› \\
\opt(‹any›)  &= ‹any› \\
\opt(‹optional(›\sigma‹)›) &= ‹optional(›\sigma‹)› \\
\opt(\sigma) &= ‹optional(›\sigma‹)›
\end{align*}
\caption{$\opt$, the function ensuring optionality/nullability of shapes}
\label{fig:opt}
\end{figure}

\begin{figure}[ht!]
\begin{gather*}
\cfs(\sigma_1 = \recd{ k_1 : v_1, \cdots, k_n : v_n }, \sigma_2 = \recd{ k_1 : v'_1, \cdots, k_n : v'_n }) = \\
\recd[\bigg]{
\forall\ k_n \in \sigma_1 \cup \sigma_2 .\ k_n : \left( \begin{array}{ll}
  \csh(v_n, v'_n) & \text{if } k_n \in \sigma_1 \cap \sigma_2 \\
  \opt(v_n) & \text{if } k_n \notin \sigma_2 \\
  \opt(v'_n) & \text{if } k_n \notin \sigma_1
\end{array} \right)
}
\end{gather*}
\caption{$\cfs$, the function for finding the common shape of two records}
\label{fig:cfs}
\end{figure}

Figure~\ref{fig:cfs} shows how the common shape of two record shapes is found by finding the common shapes of its fields. For keys that are not present in both records, the shape that is present is optional/nullable. Note that in «Types from Data», records have row types\cite{row-types} so that all records can be considered to have the same keys. To minimize the number of new concepts needed to understand the basic algorithm I have chosen this slightly less elegant notation in figure~\ref{fig:cfs} instead.

\section{Intermediary passes}

In the basic version of the algorithm, there are no intermediary passes. These passes are mainly a result of extensions of the algorithm. The extensions come about for two main reasons: Improving the code to more closely resemble hand-written code, and adding configurability of the inference and code generation.

We will look at these extensions, and their consequences for the algorithm in section~\ref{sec:extensions}.

\section{Generating Rust types}

% field and type naming and renaming

% runnable example

\subsection{Use of derivable traits}

As outlined in section~\ref{sec:derive-list} a lot of the functionality of «json_typegen» is founded on the fact that many traits be derived -- i.e. that they can be implemented by just adding their name to a list -- and that this works even for complex generated types.

The fact that this is possible relies on two important preconditions:

\begin{enumerate}
  \item That our generated types are composed of either our base types, or other generated types. I.e. that every leaf in our generated type tree is one of our base types.
  \item That every trait in our derive list is derivable and implemented for each of our base types.
\end{enumerate}

% Proof of "every leaf in our generated type tree is one of our base types" necessary?

% Proof by structural induction here of the claim that this then makes deriving safe for complex types?

Both of these preconditions can be broken by the user if the right configuration option is provided. As explained in section~\ref{sec:derive-list}, the derive list can be overridden. This can obviously break our second precondition, either if a trait is not implemented for one of our base types, or for that matter if the trait is not derivable at all. There are also ways configuration can introduce new, essentially opaque, types into the code generation. These new types essentially become new base types, and as such can easily break our first precondition.

If a user breaks our preconditions in this way and this leads to a compiler error. The messages when a derive fails are clear and should make it quite obvious to the user what the problem is. With this in mind I think letting the user break these preconditions is an acceptable trade-off for the benefit these configurations provide.

While letting the user break these preconditions is acceptable, care has to be taken to preserve these preconditions when extending the basic system. To make the source of any errors obvious, no derive errors should be possible that does not directly mention a trait or type explicitly specified by the user. E.g. if the system is extended with sets, enabling this extension should not be possible without explicitly choosing a target type, if doing so breaks the preconditions.

\section{Code generation dilemmas}
\label{sec:design-considerations}

When inferring shapes and generating code based on JSON one has to work with incomplete data. As such it is unavoidable that some choices have to be made. Unfortunately several of these choices do not present any alternative that is clearly better in all cases.

\placeholder{Option vs Enum, detection of entirely separate types}

\placeholder{Option vs Default, missing fields}

\placeholder{Extra fields}

\begin{listing}[ht!]
\begin{minted}{json}
[
  {
    "a": 1
  },
  {
    "b": 1
  }
]
\end{minted}
\caption{JSON Dilemma \#1}
\label{lst:json-dilemma-1}
\end{listing}

\begin{listing}[ht!]
\begin{minted}{rust}
struct S {
    a: Option<i32>,
    b: Option<i32>,
}

enum E {
    A { a: i32 },
    B { b: i32 },
}
\end{minted}
\caption{JSON Dilemma \#1 - Two solutions}
\label{lst:json-dilemma-1-rs}
\end{listing}

\section{Extensions}
\label{sec:extensions}

As have been mentioned earlier, there are several ways to extend this basic setup to better align with what handwritten code would look like. We will now look at a few such extensions. For the most part we will not go into the details of how these extensions interact or the full extended algorithms, for the sake of simplicity.

\placeholder{Describe (and maybe show) extension of inference to AnyOf / tagged any types.}

% Unsigned numbers

\subsection{JSON pointer hints and configuration}

As outlined in section~\ref{sec:json-pointers} we can use JSON pointers to specify configuration options and hints to the inference that are specific to just a part of the JSON sample.

Actually applying these options and hints of course require some modifications to the simple version of the algorithm.

% some in inference, some as intermediate steps, some for

% consequences of plain pointers (and array indexes), full wildcards and partial wildcards

% same_as, ids to see which types are the same

\subsection{Maps}

One common issue with JSON is that its simplicity in its number of data structures drives people to use the same data structures with different intentions as different ad-hoc data structures. Perhaps the most common such pattern is the use of JSON objects as maps.

% Should the concept of a map be explained?

While JSON has no concept of a map, maps with strings as keys can be encoded in JSON as objects, and there is no loss of fidelity inherent in this encoding. The only issue for us is that there is no good way to infer the difference between an object used to encode a structure that will persist across e.g. API calls, and an object used to encode a mapping from arbitrary keys to values.

% "Impossible" to infer, but can be hinted.
While the intention that an object is used as a map can not be directly inferred from just a sample, with inference hints from the user code using maps can still be inferred and generated.

\begin{figure}[ht!]
\begin{gather*}
\shap(‹Object(›\{ k_1 : v_1, \cdots, k_n : v_n \}‹)›, [\cdots, ‹"" use_type map›, \cdots]) = \\
‹map(›\fold(\csh, \bot, [\shap(v_1), \cdots, \shap(v_n)])‹)› \\
\shap(a, [\cdots, ‹"" use_type map›, \cdots]) = ‹error!›
\end{gather*}
\caption{Extending the hinted $\shap$ to support maps}
\label{fig:shap-map}
\end{figure}

\begin{figure}[ht!]
\begin{align*}
\csh(‹map(›a‹)›, ‹map(›b‹)›)           &= ‹map(›\csh(a, b)‹)›     & (map)
\end{align*}
\caption{Extending $\csh$ to support maps}
\label{fig:csh-map}
\end{figure}

To be able to infer maps we would first need to add a $‹map(›a‹)›$ alternative to our list of possible shapes. In the notation I have not included the key type, as JSON only supports string keys for object fields. Figure~\ref{fig:shap-map} shows how the function $\shap$ already extended with hints could be extended to infer maps. The shown rules should take priority over the existing rule matching on ‹Object›.

% Values in a map are essentially already nullable.
$\csh$ can be extended by adding a simple rule shown in figure~\ref{fig:csh-map} before the existing rule $(any)$. One may argue that map values are already nullable, in that ‹map.get()› or any equivalent will return some nullable type, and that we should thus take care to not infer a shape for the map values which could be lowered to a non-nullable shape (or rather, to lower such types when we infer them).

However, the only nullable shape we currently have that can be lowered to something else is $‹optional(›a‹)›$, which can be lowered to its wrapped shape $a$. The only way for the algorithm to infer map values that are $‹optional(›a‹)›$ is if the map sample contains explicit ‹Null› values. For a map to contain such values would be quite rare, and if it were to happen, those null values are likely to carry meaning. With these things in mind I consider the best option to be to \emph{not} lower the map value shapes.

In most programming languages there is also the consideration of which map type to use. The Rust standard library provides two map types, ‹HashMap› and ‹BTreeMap›. In addition to these alternatives there are various map types in published in crates in the Rust ecosystem. As an example, «json_typegen» itself uses a ‹LinkedHashMap› internally. From the perspective of a user, giving a hint $‹use_type›\ t$ should work with any of the types mentioned above for $t$, as well as just ‹map›, letting «json_typegen» choose the map type.

% Whatever type json_typegen chooses ends up as part of our base types.
% HashMap<String, T> implements all of Default, Debug, Clone, PartialEq, Serialize, Deserialize

% Show how we split apart use_type HashMap into use_type map for inference algo and use_type HashMap for typegen.

\subsection{Tuple types}

Another common pattern in JSON usage that the simple algorithm has quite poor support of is the use of JSON arrays as tuples.

\placeholder{Inferring types from multiple samples/sample-sets but ensuring that they can still work together. Show how this can already be done to a certain extent using \url{https://github.com/lloydmeta/frunk}}

\placeholder{Collapsing identical shapes}
