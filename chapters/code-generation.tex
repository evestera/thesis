%!TEX root = ../main.tex

\chapter{Code generation}
\label{sec:code-generation}

Once a JSON sample has been retrieved and parsed using «serde_json» into the catchall type ‹serde_json::Value›, «json_typegen» is ready to begin inference and code generation. In this chapter we will look at the details of these three stages:

\begin{itemize}
  \item First a generalized \say{shape} is inferred from the JSON values.
  \item Then this inference is enhanced in intermediary passes.
  \item Finally Rust code is generated based on the inferred shapes.
\end{itemize}

First I will present the basic version of the inference algorithm and code generation, before looking at how this basic system can be extended in various ways.

\section{Shape inference}

The shape inference is based on the algorithms used in \fsharpdata\ as presented in the paper \emph{Types from Data: Making Structured Data First-class Citizens in F\#}\cite{fsharp-types-from-data}. As done in this paper I will use the term shape for the intermediate representation of the inference data, to avoid confusion with actual programming language types.

% Describe how shapes and types differ

The basic algorithm has two main functions, $\shap$ and $\csh$.

$\shap$, shown in figure~\ref{fig:shap} takes as its input the parsed ‹serde_json::Value› and produces a shape.

\begin{figure}[ht!]
\begin{align*}
\shap(‹Null›)          &= ‹null› \\
\shap(‹Bool(›b‹)›)     &= ‹bool› \\
\shap(‹Number(›n‹)›)   &= \left\{\begin{array}{ll}
  ‹int›   & \text{if } n \in \mathbb{Z} \\
  ‹float› & \text{otherwise}
\end{array}\right.\\
\shap(‹String(›s‹)›)   &= ‹string› \\
\shap(‹Array(›[\ ]‹)›) &= [\bot] \\
\shap(‹Array(›[e_1, e_2, \cdots, e_n]‹)›) &= [\fold(\csh, \shap(e_1), [\shap(e_2), \cdots, \shap(e_n)])] \\
\shap(‹Object(›\{ k_1 : v_1, \cdots, k_n : v_n \}‹)›) &= \{ k_1 : \shap(v_1), \cdots, k_n : \shap(v_n) \}
\end{align*}
\caption{$\shap$, the function mapping JSON values to shapes}
\label{fig:shap}
\end{figure}

\begin{figure}[ht!]
\begin{align*}
\csh(a, a)               &=  a               & (eq) \\
\csh(a, \bot)            &=  a               & (bottom) \\
\csh(‹int›, ‹float›)     &= ‹float›          & (num) \\
\csh(a, ‹null›)          &= \opt(a)          & (null) \\
\csh(a, ‹optional(›b‹)›) &= \opt(\csh(a, b)) & (opt) \\
\csh([a], [b])           &= [\csh(a, b)]     & (arr) \\
\csh(a = \{ \cdots \}, b = \{ \cdots \}) &= \cfs(a, b) & (obj) \\
\csh(a, b)               &= ‹any›            & (any)
\end{align*}
\caption{$\csh$, the common shape function}
\label{fig:csh}
\end{figure}

\begin{figure}[ht!]
\begin{align*}
\opt(‹null›) &= ‹null› \\
\opt(‹any›)  &= ‹any› \\
\opt(‹optional(›a‹)›) &= ‹optional(›a‹)› \\
\opt(a) &= ‹optional(›a‹)›
\end{align*}
\caption{$\opt$, the function ensuring optionality/nullability of shapes}
\label{fig:opt}
\end{figure}

\begin{figure}[ht!]
\begin{gather*}
\cfs(a = \{ k_1 : v_1, \cdots, k_n : v_n \}, b = \{ k_1 : v'_1, \cdots, k_n : v'_n \}) = \\
\{ k_1 : \left( \begin{array}{ll}
  \csh(v_1, v'_1) & \text{if } k_1 \in a \cap b \\
  \opt(v_1) & \text{if } k_1 \notin b \\
  \opt(v'_1) & \text{if } k_1 \notin a
\end{array}\right), \cdots, k_n : \left( \begin{array}{ll}
  \csh(v_n, v'_n) & \text{if } k_n \in a \cap b \\
  \opt(v_n) & \text{if } k_n \notin b \\
  \opt(v'_n) & \text{if } k_n \notin a
\end{array}\right) \}
\end{gather*}
\caption{$\cfs$, the function for finding the common shape of two records}
\label{fig:ufi}
\end{figure}

\section{Intermediary passes}

In the basic version of the algorithm, there are no intermediary passes. These passes are mainly a result of extensions of the algorithm. The extensions come about for two main reasons: Improving the code to more closely resemble hand-written code, and adding configurability of the inference and code generation.

We will look at these extensions, and their consequences for the algorithm in section~\ref{sec:extensions}.

\section{Generating Rust types}

% field and type naming and renaming

% runnable example

\section{Code generation dilemmas}
\label{sec:design-considerations}

When inferring shapes and generating code based on JSON one has to work with incomplete data. As such it is unavoidable that some choices have to be made. Unfortunately several of these choices do not present any alternative that is clearly better in all cases.

\placeholder{Option vs Enum, detection of entirely separate types}

\placeholder{Option vs Default, missing fields}

\placeholder{Extra fields}

\begin{listing}[ht!]
\begin{minted}{json}
[
  {
    "a": 1
  },
  {
    "b": 1
  }
]
\end{minted}
\caption{JSON Dilemma \#1}
\label{lst:json-dilemma-1}
\end{listing}

\begin{listing}[ht!]
\begin{minted}{rust}
struct S {
    a: Option<i32>,
    b: Option<i32>,
}

enum E {
    A { a: i32 },
    B { b: i32 },
}
\end{minted}
\caption{JSON Dilemma \#1 - Two solutions}
\label{lst:json-dilemma-1-rs}
\end{listing}

\section{Extensions}
\label{sec:extensions}

As have been mentioned earlier, there are several ways to extend this basic setup to better align with what handwritten code would look like. We will now look at a few such extensions. For the most part we will not go into the details of how these extensions interact or the full extended algorithms, for the sake of simplicity.

\placeholder{Describe (and maybe show) extension of inference to AnyOf / tagged any types.}

\subsection{JSON pointer hints and configuration}

As outlined in section~\ref{sec:json-pointers} we can use JSON pointers to specify configuration options and hints to the inference that are specific to just a part of the JSON sample.

Actually applying these options and hints of course require some modifications to the simple version of the algorithm.

% some in inference, some as intermediate steps, some for

% consequences of plain pointers (and array indexes), full wildcards and partial wildcards

\subsection{Maps}

One common issue with JSON is that its simplicity in its number of data structures drives people to use the same data structures with different intentions as different ad-hoc data structures. Perhaps the most common such pattern is the use of JSON objects as maps.

% Should the concept of a map be explained?

While JSON has no concept of a map, maps with strings as keys can be encoded in JSON as objects, and there is no loss of fidelity inherent in this encoding. The only issue for us is that there is no good way to infer the difference between an object used to encode a structure that will persist across e.g. API calls, and an object used to encode a mapping from arbitrary keys to values.

% "Impossible" to infer, but can be hinted.
While the intention that an object is used as a map can not be directly inferred from just a sample, with inference hints from the user code using maps can still be inferred and generated.

\begin{figure}[ht!]
\begin{gather*}
\shap(‹Object(›\{ \}‹)›, [\cdots, ‹"" use_type map›, \cdots]) = ‹map(›\bot‹)› \\
\shap(‹Object(›\{ k_1 : v_1, \cdots, k_n : v_n \}‹)›, [\cdots, ‹"" use_type map›, \cdots]) = \\
‹map(›\fold(\csh, \shap(v_1), [\shap(v_2), \cdots, \shap(v_n)])‹)› \\
\shap(a, [\cdots, ‹"" use_type map›, \cdots]) = ‹error!›
\end{gather*}
\caption{Extending the hinted $\shap$ to support maps}
\label{fig:shap-map}
\end{figure}

\begin{figure}[ht!]
\begin{align*}
\csh(‹map(›a‹)›, ‹map(›b‹)›)           &= ‹map(›\csh(a, b)‹)›     & (map)
\end{align*}
\caption{Extending $\csh$ to support maps}
\label{fig:csh-map}
\end{figure}

To be able to infer maps we would first need to add a $‹map(›a‹)›$ alternative to our list of possible shapes. In the notation I have not included the key type, as JSON only supports string keys for object fields. Figure~\ref{fig:shap-map} shows how the function $\shap$ already extended with hints could be extended to infer maps. The shown rules should take priority over the existing rule matching on ‹Object›.

% Values in a map are essentially already nullable.
$\csh$ can be extended by adding a simple rule shown in figure~\ref{fig:csh-map} before the existing rule $(any)$. One may argue that map values are already nullable, in that ‹map.get()› or any equivalent will return some nullable type, and that we should thus take care to not infer a shape for the map values which could be lowered to a non-nullable shape (or rather, to lower such types when we infer them).

However, the only nullable shape we currently have that can be lowered is $‹optional(›a‹)›$ which can be lowered to $a$. The only way for the algorithm to infer map values that are $‹optional(›a‹)›$ is if the map sample contains explicit ‹Null› values. For a map to contain such values would be quite rare, and if it were to happen, those null values are likely to carry meaning. With these things in mind I consider the best option to be to \emph{not} lower the map value shapes.

In most programming languages there is also the consideration of which map type to use. The Rust standard library provides two map types, ‹HashMap› and ‹BTreeMap›. In addition to these alternatives there are various map types in published in crates in the Rust ecosystem. As an example, «json_typegen» itself uses a ‹LinkedHashMap› internally. From the perspective of a user, giving a hint $‹use_type›\ t$ should work with any of the types mentioned above for $t$, as well as just ‹map›, letting «json_typegen» choose the map type.

% Show how we split apart use_type HashMap into use_type map for inference algo and use_type HashMap for typegen.

\subsection{Tuple types}

Another common pattern in JSON usage that the simple algorithm has quite poor support of is the use of JSON arrays as tuples.

\placeholder{Inferring types from multiple samples/sample-sets but ensuring that they can still work together. Show how this can already be done to a certain extent using \url{https://github.com/lloydmeta/frunk}}

\placeholder{Collapsing identical shapes}
