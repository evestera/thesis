%!TEX root = ../main.tex

\chapter{Code generation}
\label{sec:code-generation}

Once a JSON sample has been retrieved and parsed using «serde_json» into the catchall type ‹serde_json::Value›, «json_typegen» is ready to begin inference and code generation. In this chapter we will look at the details of these three stages:

\begin{itemize}
  \item First, a generalized \say{shape} is inferred from the JSON values.
  \item Then this inference is enhanced in intermediary passes.
  \item Finally, Rust code is generated based on the inferred shapes.
\end{itemize}

First I will present the basic version of the inference algorithm and code generation, before looking at how this basic system can be extended in various ways.

\section{Shape inference}
\label{sec:shape-inference}

The shape inference is based on the algorithms used in \fsharpdata\ as presented in the paper \emph{Types from Data: Making Structured Data First-class Citizens in F\#}\cite{fsharp-types-from-data}. As done in this paper I will use the term shape for the intermediate representation of the inference data, to avoid confusion with actual programming language types.

The goal of the shape inference is to infer generalized shapes from the samples which can then later be used for generating code. These shapes are intended to be general enough to not be tied to inference from JSON -- \fsharpdata\ also does inference for CSV and XML -- and not tied to the generation of any particular type of programming language.

\subsection{Values and shapes}

The inference thus works with two types. The JSON sample, in our case parsed into a ‹serde_json::Value› -- as was shown in listing~\ref{lst:valueenum} -- and the shape type. An abstract version of ‹serde_json::Value›, with value variables \omega, and our shape type, with shape variables \sigma, can be written as follows:

\begin{align*}
\omega ::=  &\ ‹Null› \mid ‹Bool(›b‹)› \mid ‹Number(›n‹)› \mid ‹String(›s‹)› \\
       \mid &\ ‹Array(›[\omega_1, \cdots, \omega_n]‹)› \mid ‹Object(›\{ k_1 : \omega_1, \cdots, k_n : \omega_n \}‹)›\\
\\
\sigma ::=  &\ ‹any› \mid \bot \mid ‹bool› \mid ‹string› \mid ‹int› \mid ‹float› \\
       \mid &\ ‹optional(›\sigma‹)› \mid \vect{\sigma} \mid \recd{ k_1 : \sigma_1, \cdots, k_n : \sigma_n }
\end{align*}

To be clear about how notation is disambiguated: $[a, b, c]$ is an actual sequence of items, while $\vect{a}$ is the \emph{shape} representing a sequence/list. Likewise $\{ k_1 : v_1, \cdots, k_n : v_n \}$ is a map-like collection, while $\recd{ k_1 : \sigma_1, \cdots, k_n : \sigma_n }$ is the \emph{shape} representing a record/object/struct.

For an intuitive understanding of the more abstract shapes it is helpful to think of what kind of knowledge each shape represents. $\bot$ (read as \say{bottom}) represents the complete lack of knowledge about what type should be inferred. $‹optional(›\sigma‹)›$ represents the knowledge that a type may sometimes be ‹null› or a field may not always be there, i.e. that it is nullable or optional. So $‹optional(›\bot‹)›$ tells us that a type is optional, but that we know nothing else of about its type. The ‹any› shape, on the other hand, represents conflicting information. If we end up with the ‹any› shape it means that the information we have received is not representable by any of the other alternatives. E.g. a type that can be both a ‹string› and an ‹int›.

% Differences from algorithm as presented in Types from Data
Unlike in «Types from Data», to preserve information and let the choice be made in the code generation step, lists\footnote{\ldots\ and with extensions, other collections.} are not considered nullable. In other words, the inference can infer the shape $‹optional(›\vect{\sigma}‹)›$, but it is up to the code generation to decide how to handle the shape. In the default code generation, ‹Option<Vec<T>>› will not be generated, but this can be enabled by configuration (‹optional_collections›).

With this change, a separate ‹null› shape, which the original algorithm has, is no longer necessary. Instead, $\bot$ is not considered nullable either, and $‹optional(›\bot‹)›$ serves the purpose of ‹null›.

\subsection{From values to shapes}

\begin{figure}[ht!]
\begin{align*}
\shap(‹Null›)          &= ‹optional(›\bot‹)› \\
\shap(‹Bool(›b‹)›)     &= ‹bool› \\
\shap(‹Number(›n‹)›)   &= \left\{\begin{array}{ll}
  ‹int›   & \text{if } n \in \mathbb{Z} \\
  ‹float› & \text{otherwise}
\end{array}\right.\\
\shap(‹String(›s‹)›)   &= ‹string› \\
\shap(‹Array(›[\omega_1, \cdots, \omega_n]‹)›) &= \vect{ \fold(\csh, \bot, [\shap(\omega_1), \cdots, \shap(\omega_n)]) } \\
\shap(‹Object(›\{ k_1 : \omega_1, \cdots, k_n : \omega_n \}‹)›) &= \recd{ k_1 : \shap(\omega_1), \cdots, k_n : \shap(\omega_n) }
\end{align*}
\caption{$\shap(\omega)$, the function converting JSON values to shapes}
\label{fig:shap}
\end{figure}

The main inference function $\shap(\omega)$, shown in figure~\ref{fig:shap} takes as its input a value and produces a shape. Values containing other values are converted by applying $\shap$ recursively.

$\fold(\textit{function}, \textit{base}, \textit{sequence})$ is the fold operator\cite{fold-tutorial}, common in functional programming (and available on iterators in Rust). In this case, it reduces a sequence of shapes to a single shape by finding a common shape with the $\csh(\sigma_1, \sigma_2)$ function, which we will look at in the next section. The initial shape is $\bot$, which means that an empty ‹Array› will be represented as the shape $\vect{\bot}$.

\subsection{Finding common shapes}

\begin{figure}[ht!]
\begin{align*}
\csh(\sigma, \sigma)     &=  \sigma          & (eq) \\
\csh(\sigma, \bot)       &=  \sigma          & (bottom) \\
\csh(‹int›, ‹float›)     &= ‹float›          & (num) \\
\csh(\sigma_1, ‹optional(›\sigma_2‹)›) &= \opt(\csh(\sigma_1, \sigma_2))  & (opt) \\
\csh(\vect{\sigma_1}, \vect{\sigma_2}) &= \vect{\csh(\sigma_1, \sigma_2)} & (arr) \\
\csh(\sigma_1 = \recd{ \cdots }, \sigma_2 = \recd{ \cdots }) &= \cfs(\sigma_1, \sigma_2) & (obj) \\
\csh(\sigma_1, \sigma_2) &= ‹any›            & (any)
\end{align*}
\caption{$\csh(\sigma_1, \sigma_2)$, the common shape function}
\label{fig:csh}
\end{figure}

Figure~\ref{fig:csh} shows the function $\csh(\sigma_1, \sigma_2)$ which for two shapes finds a common shape which can represent both shapes.

\begin{figure}[ht!]
\begin{align*}
\opt(‹any›)                &= ‹any› \\
\opt(‹optional(›\sigma‹)›) &= ‹optional(›\sigma‹)› \\
\opt(\sigma)               &= ‹optional(›\sigma‹)›
\end{align*}
\caption{$\opt(\sigma)$, the function ensuring optionality/nullability of shapes}
\label{fig:opt}
\end{figure}

\begin{figure}[ht!]
\begin{gather*}
\cfs(\sigma_1 = \recd{ k_1 : v_1, \cdots, k_n : v_n }, \sigma_2 = \recd{ k_1 : v'_1, \cdots, k_n : v'_n }) = \\
\recd[\bigg]{
\forall\ k_n \in \sigma_1 \cup \sigma_2 .\ k_n : \left( \begin{array}{ll}
  \csh(v_n, v'_n) & \text{if } k_n \in \sigma_1 \cap \sigma_2 \\
  \opt(v_n) & \text{if } k_n \notin \sigma_2 \\
  \opt(v'_n) & \text{if } k_n \notin \sigma_1
\end{array} \right)
}
\end{gather*}
\caption{$\cfs(\sigma_1, \sigma_2)$, the function for finding the common shape of two records}
\label{fig:cfs}
\end{figure}

Figure~\ref{fig:cfs} shows how the common shape of two record shapes is found by finding the common shapes of its fields. For keys that are not present in both records, the shape that is present is optional/nullable. Note that in «Types from Data», records have row variables\cite{row-types} so that all records can be considered to have the same keys. To minimize the number of new concepts needed to understand the basic algorithm I have chosen this slightly less elegant notation in figure~\ref{fig:cfs} instead.

\section{Intermediary passes}

In the basic version of the algorithm, there are no intermediary passes. These passes are mainly a result of extensions of the algorithm. The extensions come about for two main reasons: Improving the code to more closely resemble hand-written code, and adding configurability of the inference and code generation.

We will look at these extensions and their consequences for the algorithm in section~\ref{sec:extensions}.

\section{Generating Rust types}

% field and type naming and renaming

% runnable example

\subsection{Use of derivable traits}
\label{sec:use-of-derivable-traits}

As outlined in section~\ref{sec:derive-list} a lot of the functionality of «json_typegen» is founded on the fact that many traits be derived -- i.e. that they can be implemented by just adding their name to a list -- and that this works even for complex generated types.

The fact that this is possible relies on two important preconditions:

\begin{enumerate}
  \item That our generated types are composed of either our base types, or other generated types. I.e. that every leaf in our generated type tree is one of our base types.
  \item That every trait in our derive list is derivable and implemented for each of our base types.
\end{enumerate}

% Proof of "every leaf in our generated type tree is one of our base types" necessary?

% Proof by structural induction here of the claim that this then makes deriving safe for complex types?

Both of these preconditions can be broken by the user if the right configuration option is provided. As explained in section~\ref{sec:derive-list}, the derive list can be overridden. This can obviously break our second precondition, either if a trait is not implemented for one of our base types, or for that matter if the trait is not derivable at all. There are also ways configuration can introduce new, essentially opaque, types into the code generation. These new types essentially become new base types, and as such can easily break our first precondition.

If a user breaks our preconditions in this way and this leads to a compiler error. The messages when a derive fails are clear and should make it quite obvious to the user what the problem is. With this in mind, I think letting the user break these preconditions is an acceptable trade-off for the benefit these configurations provide.

While letting the user break these preconditions is acceptable, care has to be taken to preserve these preconditions when extending the basic system. To make the source of any errors obvious, no derive errors should be possible that does not directly mention a trait or type explicitly specified by the user. E.g. if the system is extended with sets, enabling this extension should not be possible without explicitly choosing a target type if doing so breaks the preconditions.

\section{Code generation dilemmas}
\label{sec:design-considerations}

When inferring shapes and generating code based on JSON one has to work with incomplete data. As such it is unavoidable that some choices have to be made. Unfortunately several of these choices do not present any alternative that is clearly better in all cases.

\placeholder{Option vs Enum, detection of entirely separate types}

\placeholder{Option vs Default, missing fields}

\placeholder{Extra fields}

\begin{listing}[ht!]
\begin{minted}{json}
[
  {
    "a": 1
  },
  {
    "b": 1
  }
]
\end{minted}
\caption{JSON Dilemma \#1}
\label{lst:json-dilemma-1}
\end{listing}

\begin{listing}[ht!]
\begin{minted}{rust}
struct S {
    a: Option<i32>,
    b: Option<i32>,
}

enum E {
    A { a: i32 },
    B { b: i32 },
}
\end{minted}
\caption{JSON Dilemma \#1 - Two solutions}
\label{lst:json-dilemma-1-rs}
\end{listing}

\section{Extensions}
\label{sec:extensions}

As have been mentioned earlier, there are several ways to extend this basic setup to better align with what handwritten code would look like. We will now look at a few such extensions. For the sake of simplicity, we will mostly not go into the details of how these extensions interact or the complete extended algorithms, focusing instead on each extension by itself.

\placeholder{Describe (and maybe show) extension of inference to AnyOf / tagged any types.}

% Unsigned numbers

\subsection{JSON pointer hints and configuration}
\label{sec:ext-json-pointers}

As outlined in section~\ref{sec:json-pointers} we can use JSON pointers to specify configuration options and hints to the inference that are specific to just a part of the JSON sample.

Actually applying these options and hints require (unsurprisingly) some modifications to the basic version of the algorithm.

% some in inference, some as intermediate steps, some for

% consequences of plain pointers (and array indexes), full wildcards and partial wildcards

% same_as, ids to see which types are the same

\subsection{Maps}
\label{sec:ext-maps}

One common issue with JSON is that its simplicity in its number of data structures drives people to use the same data structures with different intentions as different ad-hoc data structures. Perhaps the most common such pattern is the use of JSON objects as maps.

% Should the concept of a map be explained?

While JSON has no concept of a map, maps with strings as keys can be encoded in JSON as objects, and there is no loss of fidelity inherent in this encoding. The only issue for us is that there is no good way to infer the difference between an object used to encode a structure that will persist across e.g. API calls, and an object used to encode a mapping from arbitrary keys to values.

% "Impossible" to infer, but can be hinted.
While the intention that an object is used as a map can not be directly inferred from just a sample, with inference hints from the user code using maps can still be inferred and generated.

\begin{figure}[ht!]
\begin{gather*}
\shap(‹Object(›\{ k_1 : \omega_1, \cdots, k_n : \omega_n \}‹)›, [\cdots, ‹"" use_type map›, \cdots]) = \\
‹map(›\fold(\csh, \bot, [\shap(\omega_1), \cdots, \shap(\omega_n)])‹)› \\
\shap(\omega, [\cdots, ‹"" use_type map›, \cdots]) = ‹error!›
\end{gather*}
\caption{Extending the hinted $\shap$ to support maps}
\label{fig:shap-map}
\end{figure}

\begin{figure}[ht!]
\begin{align*}
\csh(‹map(›\sigma_1‹)›, ‹map(›\sigma_2‹)›) &= ‹map(›\csh(\sigma_1, \sigma_2)‹)›     & (map)
\end{align*}
\caption{Extending $\csh$ to support maps}
\label{fig:csh-map}
\end{figure}

To be able to infer maps we would first need to add $‹map(›\sigma‹)›$ as an alternative to our list of possible shapes. In the notation I have not included the key type, as JSON only supports string keys for object fields, and as such to assume strings as map keys in the generated code should suffice. Figure~\ref{fig:shap-map} shows how the function $\shap$ already extended with hints could be extended to infer maps. The shown rules should take priority over the existing rule matching on ‹Object›.

% Values in a map are essentially already nullable.
$\csh$ can be extended by adding a simple rule shown in figure~\ref{fig:csh-map} before the existing rule $(any)$. One may argue that map values are already nullable, since ‹map.get()› or any equivalent will return some nullable type, and that we should thus take care to not infer a shape for the map values which could be lowered to a non-nullable shape (or rather, to lower such types when we infer them).

However, the only nullable shape we currently have that can be lowered to something else is $‹optional(›a‹)›$, which can be lowered to its wrapped shape $a$. The only way for the algorithm to infer map values that are $‹optional(›a‹)›$ is if the map sample contains explicit ‹Null› values. For a map to contain such values would be quite rare, and if it were to happen, those null values are likely to carry meaning. With these things in mind, I consider the best option to be to \emph{not} lower the map value shapes.

In most programming languages there is also the consideration of which map type to use. The Rust standard library provides two map types, ‹HashMap› and ‹BTreeMap›. In addition to these alternatives there are various map types in published in crates in the Rust ecosystem. As an example, «json_typegen» itself uses a ‹LinkedHashMap› internally. From the perspective of a user, giving a hint $‹use_type›\ t$ should work with any of the types mentioned above for $t$, as well as just ‹map›, letting «json_typegen» choose the map type.

For the default map type, whatever we choose ends up as essentially a new base type. Both ‹HashMap› and ‹BTreeMap› are good candidates, but as default I have chosen ‹HashMap›, as it is recommended when \say{You want a map, with no extra functionality}\cite[std::collections]{rust-std-docs}.

% Whatever type json_typegen chooses ends up as part of our base types.
% HashMap<String, T> implements all of Default, Debug, Clone, PartialEq, Serialize, Deserialize

% Show how we split apart use_type HashMap into use_type map for inference algo and use_type HashMap for typegen.

\subsection{Tuple types}
\label{sec:ext-tuples}

Another common pattern in JSON usage that the simple algorithm has quite poor support of is the use of JSON arrays as tuples.

\placeholder{Inferring types from multiple samples/sample-sets but ensuring that they can still work together. Show how this can already be done to a certain extent using \url{https://github.com/lloydmeta/frunk}}

\subsection{Combining identical subtypes}

Finding identical subtypes is the task of finding identical subtrees in our shape tree. A naive approach needs $\binom{n}{2} = \frac{n(n-1)}{2} \in O(n^2)$ comparisons, so care must be taken on implementation. Our shape tree can be considered to be a rooted unordered labelled tree. Finding identical types would still be somewhat useful even if we consider the tree ordered, so this could be done to make initial implementation easier.
