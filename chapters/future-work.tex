%!TEX root = ../main.tex

\chapter{Conclusions and future work}
\label{chap:future-work}

\begin{picture}(0,0)
\put(160,95){\hbox{\includegraphics[width=5cm, angle=0, trim=20 20 20 20, clip]{ferris/longlist}}}
\end{picture}
\vspace{-1cm}

% Runtime library?

In this final chapter, I will look at the state and future of «json_typegen» itself, as well as how the ideas explored in this project can be continued further.

\section{Implementation state}
\label{sec:implementation-state}

While the core of «json_typegen» -- and several of the configuration options and extensions outlined in this thesis -- can be considered complete, there is a tremendous amount of features and extensions that \emph{could} be implemented. As «json_typegen» is a project I have use for myself, and I see (from the very limited logging on my server, the statistics on the official crate repository\footnote{\url{https://crates.io}} and the direct feedback I have received) that it is also of interest to others, I will continue to implement many of these features and extensions to the project myself.\footnote{Though I \emph{do} hope this thesis may also be of use to any potential contributors in the future.}

The table included below shows the current state of the implementation. At its current state it clearly shows the cost of multiple interfaces. Several extensions are implemented, but are only available through direct invocation of the code generation, as the necessary interface code is extensive and time consuming.

\newcommand\ok{\checkmark}
\begin{center}
\renewcommand\arraystretch{1.2}
\begin{tabular}{l r c c c c c}
\textbf{Feature}          & \textbf{Relevant sections} & \rotatebox{45}{\textbf{Inference}}\kern-25pt & \rotatebox{45}{\textbf{Codegen}}\kern-25pt & \rotatebox{45}{\textbf{Macro}}\kern-15pt & \rotatebox{45}{\textbf{CLI}}\kern-5pt & \rotatebox{45}{\textbf{Web}}\kern-10pt \\ \hline
Basic functionality       & \ref{chap:project-presentation}                          & \ok & \ok & \ok & \ok & \ok \\
Type visibility           & \ref{sec:visibility}                                     & \ok & \ok & \ok & \ok & \ok \\
Field visibility          & \ref{sec:visibility}                                     & \ok & \ok & -   & -   & -   \\
Derive list               & \ref{sec:derive-list}, \ref{sec:use-of-derivable-traits} &     & \ok & -   & -   & \ok \\
Nullable collections      & \ref{sec:values-and-shapes}                              &     & \ok & -   & -   & -   \\
Missing fields            & \ref{sec:strictness}                                     &     & \ok & -   & -   & -   \\
Unknown fields            & \ref{sec:strictness}                                     &     & \ok & -   & -   & -   \\
Tagged ‹any› types        & \ref{sec:tagged-any}                                     & -   & -   &     &     &     \\
JSON Pointers             & \ref{sec:json-pointers}, \ref{sec:ext-json-pointers}     & \ok & -   & -   & -   & -   \\
‹use_type›: u64, i8, etc. & \ref{sec:unsigned}                                       & -   & -   & -   & -   & -   \\
‹use_type›: shape         & \ref{sec:ext-json-pointers}                              & -   &     &     &     &     \\
‹use_type›: opaque        & \ref{sec:ext-json-pointers}                              & -   & -   &     &     &     \\
‹use_type›: map           & \ref{sec:ext-maps}                                       & \ok & \ok &     &     &     \\
‹use_type›: tuple         & \ref{sec:ext-tuples}                                     & \ok & \ok & -   & -   & -   \\
‹type_name›               & \ref{sec:json-pointers}                                  & -   & -   &     &     &     \\
‹same_as›                 & \ref{sec:json-pointers}                                  & -   & -   &     &     &     \\
Detection of equal types  & \ref{sec:combining-identical}                            & -   & -   &     &     &     \\
Macro syntax input        & \ref{sec:improving-synergy}                              &     &     &     & -   & -   \\
Macro syntax output       & \ref{sec:improving-synergy}                              &     &     &     & -   & -   \\
\multicolumn{7}{c}{} \\
\multicolumn{7}{c}{\textbf{Legend:} Complete: \ok\  Missing: -\ \  Not applicable: \ \ \textit{(blank)}}
\end{tabular}
\end{center}

Despite how clear this cost is, I am convinced the benefits of having these interfaces outweighs the cost, and that multiple interfaces is the strongest benefit «json_typegen» has over a pure type-provider-like interface. Each interface is useful in itself even without the synergy, and having to rewrite the (far from trivial) code generation for each interface would make it all the more likely that an interface might not exist at all.

As mentioned in section~\ref{sec:cost-of-configurability}, it may be possible to generate interface implementations, at least to a certain extent. If this is achieved, both extending «json_typegen» itself and expanding to other formats beside JSON should be substantially easier.

\section{Expanding to other formats}

While JSON is perhaps the most popular serialization format at the moment, it is obviously far from the only one. \fsharpdata, which was the main inspiration for «json_typegen», has type providers for XML, HTML and CSV in addition to JSON. These type providers have a common core of inference and type generation logic. While some of the choices and extensions of «json_typegen» are somewhat specific to JSON, like \fsharpdata, it should be possible to use the same core for multiple formats. For types that have both a «serde» implementation and either a ‹Value› type of its own, or somehow can map into ‹serde_json::Value›, adding basic support for the format should be somewhat straight forward. For some formats it may however be better to create entirely separate libraries, as not every format maps straightforwardly to the set of shapes used by «json_typegen».

\section{JSON Schema}

JSON itself is a simple, schemaless data format. There is however a separate schema format -- JSON Schema\footnote{\url{http://json-schema.org/}} -- that is in development (and has been for some years). It is likely that similar code generation techniques to those used in this project could be applied to JSON Schema, and it may seem like a natural progression from \say{types from samples} is \say{types from schemas}. There are however some issues with JSON Schema that make me a bit hesitant to start (or recommend) a code generation project for JSON Schema:

\begin{itemize}
  \item While it is only a personal observation, it seems to me as though a lot of the people/websites actually using JSON Schema (often in Swagger/OpenAPI) don't actually use it according to the specification. As an example; many completely omit the "required" field that specify which fields of an object are not nullable. In my opinion this defeats some of the point of a schema, as it leaves us unable to generate code that is both safe and ergonomic without inspecting the data itself and making assumptions based on it.
  \item JSON Schema also has some challenges for code generation in that it can easily represent types which are very hard to represent with «serde»-compatible types, so it may require actually writing the deserialization code more or less specifically for any hypothetical JSON Schema library. The references in JSON Schema also means that a simple, recursive tree walk is not enough to parse a schema into shapes.
  \item The specification also lacks straightforward ways to represent usage patterns that are already common in JSON, like objects as maps and arrays as tuples.
\end{itemize}

Another interesting possibility is the use of the inferred shapes of «json_typegen» to \emph{create} JSON schemas. While this would certainly be possible, I am a little skeptical of it's usefulness. However, as it should be a rather straightforward addition of another output format to «json_typegen», even it is only slightly useful, that may be enough for someone to do it when the inference is already in place.

\section{Towards a true type provider experience for Rust}

In closing, I want to touch on the question of why languages with full support for type providers are as rare as they are. As explained in section~\ref{sec:type-provider-implementation}, support for type providers is only two relatively simple to understand concepts: The ability to create types based on some external resource, and the ability to delay the creation of parts of these types via e.g. thunks.

While full procedural macros are not all that common, they are not exceedingly rare either. A simple version of procedural macros can be implemented with a simple pre-processing step. Especially if they are only to be used for basic type provider support. For most type providers, a simple string or two is enough input to do something useful. The fact that procedural macros are in a sense external to the core of a language is also part of why tooling support for procedural macros lag behind. They have to be expanded, evaluated, eliminated, for tools to do their job.

Delayed evaluation of parts of types, however, is something that is much rarer, as this concept has effects into how the very type system is evaluated inside a compiler. So, would this even be possible in Rust? I'm not sure. If one were to explore this, a good starting point would perhaps be to implement a toy language which does little more than support thunks in types.

However, I believe both \fsharpdata\ and «json_typegen» show that this second part of type provider support is not essential to create something useful. And while it may not be necessary to add a code generation tool onto every compiler, for a language such as Rust, which have the necessary tools for basic type providers already available, a future where projects that look like type providers are exceedingly common seems almost inevitable. As soon as tooling catches up.

\begin{center}
\vspace*{8cm}
\includegraphics[width=6cm, angle=0, trim=10 10 10 10, clip]{ferris/telescope}
\end{center}
