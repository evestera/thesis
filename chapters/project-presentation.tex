%!TEX root = ../main.tex

\chapter{Presentation of the project}

Type providers have significant benefits that are worth exploring in other programming languages. I have created a project that aims to approximate the benefits of type providers in Rust.

The project is divided into five crates\footnote{A crate is the equivalent of a library or a package in Cargo, Rust's package manager}, as well as a crate demonstrating the use of the main crate. Most important of these are one library -- a procedural macro -- and two binaries -- a command line interface and a web interface. Figure~\ref{fig:crates} shows how the crates depend on each other.

\begin{figure}[ht!]
\centering
\begin{tikzpicture}[>=stealth, thick, shorten >=1pt]
\graph [layered layout, grow=up, level distance=1.3cm, nodes={draw, rectangle, rounded corners}] {
"json\_typegen\_demo" -> "json\_typegen";
"json\_typegen" -> "json\_typegen\_derive";
"json\_typegen\_derive" ->[shorten >=6pt] "json\_typegen\_shared";
"json\_typegen\_cli" -> "json\_typegen\_shared";
"json\_typegen\_web" ->[shorten >=6pt] "json\_typegen\_shared";
};
\end{tikzpicture}
\caption{Internal dependencies in the project}
\label{fig:crates}
\end{figure}

\section{The procedural macro}

The crate «json_typegen» provides a procedural macro of the same name, ‹json_typegen›. This macro provides an interface very similar to the type providers from \fsharpdata.

\begin{listing}[ht!]
\begin{minted}{rust}
#[macro_use]
extern crate json_typegen;
extern crate serde_json;

json_typegen!("Point", r#"{ "x": 1, "y": 2 }"#);

fn main() {
    let mut p: Point =
        serde_json::from_str(r#"{ "x": 3, "y": 5 }"#).unwrap();
    println!("deserialized = {:?}", p);
    p.x = 4;
    let serialized = serde_json::to_string(&p).unwrap();
    println!("serialized = {}", serialized);
}
\end{minted}
\caption{Usage of the procedural macro}
\label{lst:typegenmacro}
\end{listing}

In listing~\ref{lst:typegenmacro} a minimal example of usage of the procedural macro ‹json_typegen› is shown. This example is provided in the project source as a demo crate, «json_typegen_demo». It is worth noting that the calls to ‹unwrap› unwraps result values assuming success, and will crash the program in the event of a serialization or deserialization failure. In a real world use case we would replace these calls with error handling code.

As can be seen in the example, the procedural macro defines a type -- ‹Point› -- which is then available to be used by the programmer as any other type. The type gives type safe access to its fields, in the example ‹p.x›. Attempts to access an invalid field -- e.g. ‹p.z› -- would be a type error and thus be caught at compile time.

The procedural macro supports inline samples as shown, samples stored as local files and URLs that point to remote samples.

\subsection{Limitations compared to an actual type provider}
\label{sec:macro-limitations}

While the procedural macro looks like and in many ways works like a type provider, it is not a real type provider. For some features actual support for type providers in the compiler and the tooling would be necessary.

At the moment there is no tooling solution for Rust that provides autocomplete for types generated by procedural macros. There is reason to believe that this may change in the future. However, if this will happen at all, if it would work for the ‹json_typegen› macro specifically and when it would hypothetically be done is all mostly guesswork at this point.

Since this solution is only a procedural macro that is run at compile time and does not have knowledge of the rest of the program there is no way to implement code generation on demand that depends on how the type is used. A version of this feature would be possible to do with a procedural macro system, but would require making significant changes to the Rust compiler. It is also very possible that it is not desirable to change the macro system in this manner.

\subsubsection{Procedural macro hack}

Another limitation of the current implementation is that the macro can only be used once per scope. This is a consequence of the fact that, at the time of writing this, function-like procedural macros are currently not enabled on the stable version of the Rust compiler.

To work around this limitation ‹json_typegen› is actually a normal rule-based macro. The invocation of this macro expands to the declaration of a type that uses a custom derive that is implemented in «json_typegen_derive»\footnote{This hack is demonstrated in isolation, and described in more detail at \url{https://github.com/dtolnay/proc-macro-hack}}. Since the ordinary macro is unable to create new type names for each invocation, two invocations of the macro would create two (unused) types with the same name.

\begin{listing}[ht!]
\begin{minted}{rust}
mod point {
    json_typegen!("pub Point", "point_sample.json");
}

mod vector {
    json_typegen!("pub Vector", "vector_sample.json");
}

use point::*;
use vector::*;
\end{minted}
\caption{Workaround for the procedural macro hack}
\label{lst:hack-workaround}
\end{listing}

In my testing thus far I have not had a need to work around this limitation, but if necessary the easiest way is to wrap the macro invocation in module scopes, and if desired import the created types. Another workaround is to use «json_typegen_derive» directly. However in the near future\footnote{Tracking issue for procedural macros: \url{https://github.com/rust-lang/rust/issues/38356}} no hack should be needed at all as function-like procedural macros become available on the stable compiler.

\section{The command line interface}

\placeholder{Command line example}

The crate «json_typegen_cli» provides a binary, ‹json_typegen›, which is a command line interface to the same code generation as is used in the procedural macro.

% Diffing of earlier code with new

\section{The web interface}

\placeholder{Screenshot of web interface}

The procedural macro and the command line interface both require the user to download and compile a somewhat significant amount of code. In small projects where the sample can be assumed not to change this initial cost may for a lot of users seem to outweigh the benefits of the code generation.

Both of these kinds of interfaces are also quite bad interface-wise if the user wants to use a large number of configuration options. To counteract both of these problems I have made a web interface as a third way to use access the code generation logic.

The crate «json_typegen_web» provides a binary that both provides a web API for the code generation, as well as hosting the static HTML/JavaScript files providing a frontend to this API.

\section{Shared code}

As seen in figure~\ref{fig:crates} the macro, web interface and CLI all depend on a common crate, «json_typegen_shared». This crate contains the actual inference and code generation logic. The following data flow is common to all three interfaces:

\begin{enumerate}
  \item Get the actual JSON text for the sample.
  \item Parse the JSON text into JSON values
  \item Infer type shapes from the JSON values
  \item Generate Rust code from the type shapes
\end{enumerate}

As mentioned in section~\ref{sec:macros} procedural macros are $ TokenStream \rightarrow TokenStream $ functions. However a ‹TokenStream› can both be parsed from a string, and written into a string. In other words we have access to both a $ String \rightarrow TokenStream $ and a $ TokenStream \rightarrow String $ function.

The different interfaces we want to provide place some requirements on what signatures we need to be able to use our code with. As mentioned, procedural macros need to have the signature $ TokenStream \rightarrow TokenStream $. A web API on the other hand must at least externally have an interface that boils down to $ String \rightarrow String $. That leaves us with two alternatives, (using $\rightarrow$ for a conversion and $\Rightarrow$ for the transformation):

\begin{align*}
String \rightarrow TokenStream &\Rightarrow TokenStream \rightarrow String & \text{web} \\
                   TokenStream &\Rightarrow TokenStream                    & \text{macro} \\
\\
                        String &\Rightarrow String                         & \text{web} \\
TokenStream \rightarrow String &\Rightarrow String \rightarrow TokenStream & \text{macro}
\end{align*}

In the real project things are a bit more complicated, as more types are involved, but the central point that we can consolidate different interfaces to a shared core that does the actual transformation thanks to conversion methods still stands.

\section{Synergy}
% Demonstrate how CLI or web interface can be used together with proc-macro. Explain how this then gives autocomplete etc.

Having these different interfaces provides us with some benefits that would be very difficult or impossible to achieve with only a single one.

% migration
With the three interfaces, there is a clear migration path for most use cases. A user can start with the web interface to test the code generation and see if the results fit their use case. Since it is just a normal website they can do this without having to install anything or add any dependencies.

If the code generation is suitable to their use case, copying and pasting from a website can get tedious. That combined with the additional benefits of the procedural macro will likely lead people to try the procedural macro.

As mentioned in section~\ref{sec:macro-limitations} however, the procedural macro has some limitations, and as mentioned in section~\ref{sec:disadvantages-of-type-providers} even real type providers have their disadvantages. With a combination of the interfaces in this project however, interesting solutions or workarounds for several of these issues are possible.

Perhaps the most obvious concern that is no longer an issue is the fact that it is not possible to see expanded code with the type providers from \fsharpdata\ and the procedural macro in «json_typegen». The two additional interfaces in this project both make it easy to see the generated code.\footnote{On nightly versions of Rust it is in fact also possible to see the resulting code after macro expansion. However, since it is fully expanded it is primarily useful for debugging purposes.}
% (also mention cargo-expand here)

The two additional interfaces also makes lock-in almost a non-issue. If a user wants to stop using «json_typegen» entirely, they can just generate the code once using either the web or command line interface and replace the macro invocation with the generated code.

% autocomplete by generated code
One reason why someone might want to not use the procedural macro at the moment is the lack of autocomplete it currently is accompanied by. However, switching from the procedural macro to manually generated code means abandoning the verification against the external resource that the procedural macro gives.

\subsection{Conditional compilation}

% autocomplete by generated code, but keeping benefits
To make the generated source available to editors while still keeping the benefits of the procedural macro it is possible to use conditional compilation. Listing~\ref{lst:conditional-compilation} shows an example setup where pre-generated code is used in the default build, but the pre-generated code is ignored and the procedural macro used instead when a compilation flag is enabled.

\begin{listing}[ht!]
\textbf{In the source code:}
\begin{minted}{rust}
#[cfg(not(feature = "online-samples"))]
#[derive(Default, Debug, Clone, Serialize, Deserialize)]
struct Point {
    x: i64,
    y: i64,
}

#[cfg(feature = "online-samples")]
json_typegen!("Point", "http://example.com/point.json");
\end{minted}
\vspace{5mm}

\textbf{Normal build:}
\begin{minted}{bash}
$ cargo build
\end{minted}
\vspace{5mm}

\textbf{Type checking against online samples:}
\begin{minted}{bash}
$ cargo check --features "online-samples"
\end{minted}
\caption{Conditional compilation}
\label{lst:conditional-compilation}
\end{listing}

% require network access only for verification
Using conditional compilation in this way also lets us get around the issue of requiring network access to build. If network access is the only issue we care about we don't even need to pre-generate the code, and can instead use two macro invocations -- one with a local or inline sample and one with the remote one -- as our two options.

% separate build for verification, so "normal" build still reproducible
Since the normal build in such a setup does not rely on an external resource we also get reproducible builds this way. Even if the external resource changes, it will still be possible to check out an old version of our source code and compile it. This can actually be very useful e.g. for tracking down bugs with tools like ‹git bisect›.

\section{Configurability}

There is one final problem that does not just apply to type provider like tools, but to code generation in general: Code generation always has to make some assumptions, and thus do not cover every use case and every requirement a programmer might have.

\begin{listing}[ht!]
\textbf{Sample:}
\begin{minted}{json}
{
  "registered": "2016-08-19",
  "..."
}
\end{minted}
\vspace{5mm}

\textbf{Inferred type:}
\begin{minted}{rust}
#[derive(...)]
struct Order {
    registered: String,
    ...
}
\end{minted}
\vspace{5mm}

\textbf{Desired type:}
\begin{minted}{rust}
#[derive(...)]
struct Order {
    registered: Date,
    ...
}
\end{minted}
\caption{A simple example of how generated code can fall short}
\label{lst:string-date}
\end{listing}

Listing~\ref{lst:string-date} shows a simple example of a case where the inferred type falls short of what the programmer would write by hand. There is no ‹Date› type in the Rust standard library, and as such, even with inspection of the ‹String› it would not be natural for the code generation to infer a ‹Date› type by default. While the ‹Date› type specifically may conceivably be added to the standard library in the future, the same example could apply to all manner of domain specific types.

In such cases where the generated code falls short the programmer has a few alternatives:

\begin{itemize}
  \item They can simply use the generated code as is, and convert from ‹String› to ‹Date› as necessary. If the suboptimal type is used frequently in the code though, littering the code with snippets like ‹Date::from(order.registered)› becomes frustrating pretty fast.
  \item They can customize the generated code by hand, and reapply the customizations whenever the code has to be generated again. Since this requires access to the generated code this alternative means abandoning any procedural macro or type provider. As it also makes the customized code incompatible with the generated code conditional compilation will not help here either.
  \item Finally, they can customize the generated code by hand, and completely abandon the code generation tool.
\end{itemize}

All of these alternatives have significant downsides, so if configuration of the code generation is possible, and does not come with significant downsides of its own it would in most cases be clearly preferable.

While it is clear that «json_typegen» should have at least \emph{some} configuration options, it is not obvious to what extent. It is probably not a good idea to try to cover every edge case, as such a goal could quickly lead the code to become too complex to maintain and expand. We can however make an effort to cover the most common cases, and thus increase the usefulness of the project for most people.

In the next sections I will discuss the configuration options currently available in «json_typegen» and some proposals for future expansions of these options.

\subsection{Visibility}

Perhaps the simplest configuration option in «json_typegen» is type visibility. By default the generated types have no visibility specification. In Rust this means that the types are private, i.e. only accessible from the module\footnote{A file in Rust is itself a module} they are defined in, and any modules it may contain.

Private visibility for types from «json_typegen» was chosen as the default because of the inherent volatility of generating types from external resources and the fact that any change to a public type is considered a breaking API change. Thus, if the types were public by default, any change in an external sample would cause a breaking API change in users crates. If this is what the users desires, they should be free to choose so, but it is not something that should happen by accident.

To set a more public visibility for the generated types, a visibility specifier may be given along with the name for the root type. E.g. ‹json_typegen!("pub Point", ...)› would create the type ‹Point› as a public type, and if the type name ‹"pub(crate) Point"› was given, it would be created as a type visible within the current crate.

By default in «json_typegen» struct fields \say{inherit} the visibility specifier (if any) of the containing struct. This should usually be the desired behavior, but if not it can be overridden with a visibility specifier set using the option ‹field-visibility›.

\subsection{Derive list}

As explained in section~\ref{sec:traits} common behavior is specified in traits, and many of these traits can be derived -- automatically implemented -- by annotating the type with a list of traits to derive.

By default «json_typegen» annotates each generated type with the following derive list:

\begin{minted}{rust}
#[derive(Default, Debug, Clone, Serialize, Deserialize)]
\end{minted}

\morestuff

\subsection{JSON pointers}

While some configuration options like type visibility and the derive list make sense to specify globally, for the whole JSON document at once, a lot of what would be beneficial to configure has to be done on a much more granular level.

The initial case in listing~\ref{lst:string-date}, where we would want the type of a specific field to be ‹Date› rather than the inferred ‹String› is an example of where we would need such granularity.

In the example we would want to target \say{the field ‹registered› of the root object}. In our original JSON example of section~\ref{sec:json} we extracted \say{the field ‹areaCode› of the first element of the ‹phoneNumbers› field of the root object}. One of the ways we did this was with the ‹pointer› method from «serde_json» which takes as an argument something called a JSON pointer.

The JSON Pointer specification\cite{RFC6901} was originally developed along with the specification for JSON Patch\cite{RFC6902} for the HTTP PATCH method, which needed a way to specify specific elements of a JSON document. A JSON pointer is simply a string consisting of \say{reference tokens} each prefixed with a forward slash (to separate the tokens). A reference token is interpreted as either a field name or an array index, depending on whether an object or an array is encountered. So to specify \say{the field ‹registered› of the root object} we would write the JSON pointer ‹"/registered"›. And the JSON pointer we used in section~\ref{sec:json} to extract \say{the field ‹areaCode› of the first element of the ‹phoneNumbers› field of the root object} was ‹"/phoneNumbers/0/areaCode"›.

Since JSON pointers are representable as simple strings they are easy to use as arguments to both macros or in command line invocations. Since they are already used by «serde_json» it also makes sense to use JSON pointers rather than introduce yet another way to reference data originating from JSON.

While JSON pointers are a \say{stringly typed} way to talk about data, it may seem strange to use it in a project that is so focused on type safety. There is however a significant distinction in using such code at compile time and at runtime. The criticism of \say{stringly typed} code in a strongly typed language is that it ends up reverting to a state where errors are discovered at runtime. If configuration for «json_typegen» specified through a JSON pointer does not apply to any of the sample, this can be known well before runtime, when generating the code (i.e. at compile time for the macro), and a warning or an error can be shown to the user at that point in time.

% Actually show some examples of using JSON pointers for configuration

Due to the use case the JSON pointer specification was developed for, there is however a rather glaring omission when it comes to adapting it to our use case. The complete lack of any wildcard.

% Existing solutions to lack of wildcard

\placeholder{Configuration/inference hints through JSON pointers}

\subsection{Cost of configurability}

% Problems with adding a lot of configuration in a project with multiple interfaces

% Approaches to working around this by generating parts of the interfaces based on the Config data type.

\section{Improving the synergy}

While the synergy between the different interfaces is already quite good there are ways in which it could be improved.

Since many of the use cases involve moving between the interfaces, making the transition between them as smooth as possible is an important part of the total user experience of the project.

While the "native" user experiences of the interfaces are quite different, with different strengths and weaknesses, the central functionality they provide, including configurability, should be the same. However if a user have taken extensive advantage of this configurability, having to replicate the same configuration settings from one interface in another can erase the convenience the other interface would provide.

One way to get around this problem is to have the interfaces have some common way to specify configuration. However, forcing users to type in a configuration in a text field in a web interface, instead of using a proper form is clearly a horrible user experience. What I propose instead is to make the web interface and CLI able to read configuration settings from the macro syntax, using the macro syntax as an ad hoc data interchange format for configuration.

% Easy way to transition between interfaces. E.g. copy-paste macro into web interface to see result. Output macro in web interface that is equivalent to the current form input. Etc.
% Doing this would mean essentially writing a minimal macro parser and generator. I.e.
% MacroString -> (SampleSource, Config)
% (SampleSource, Config) -> MacroString

% Have CLI be able to set up conditional compilation. Maybe also web interface?

\placeholder{Describe idea behind \url{https://github.com/evestera/atom-json-typegen} and superficially how it works/can work.}
